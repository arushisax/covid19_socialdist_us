---
title: "tweet_sentanalysis"
author: "Arushi Saxena"
date: "4/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(twitteR)
library(rtweet)
library(SnowballC)
library(tm)
library(syuzhet)
library(maps)
library(ggthemes)
library(revgeo)
library(mgsub)
library(tidytext)
install.packages("cowplot") #to use save_plot function
library(cowplot)
```

```{r Sentiment Analysis - Chart 1}
# Create main df for senti analysis
s_df <- read_csv("Shiny_app/tweets_0412_v2.csv")

# Transform Twitter sentences into words
data_tibble <- s_df %>%
  unnest_tokens(output = "words", input = text, token = "words")

# Remove stop words from the text data
virus_tibble_clean <- data_tibble %>%
  anti_join(stop_words, by=c("words"="word"))

data_tidy_sentiment <- virus_tibble_clean %>% 
  # Inner join to bing lexicon by term = word
  inner_join(get_sentiments("bing"), by = c("words" = "word")) %>% 
  # Count by term and sentiment, weighted by count
  count(words, sentiment) %>%
  # Spread sentiment, using n as values
  spread(sentiment, n, fill = 0) %>%
  # Mutate to add a polarity column
  mutate(Polarity = positive - negative)

summary(data_tidy_sentiment)
data_tidy_sentiment

data_tidy_pol <- data_tidy_sentiment %>% 
  # Filter for absolute polarity at least 85
  filter(abs(Polarity) >= 85) %>% 
  # Remove swear words
  filter(!(words=="fuck"|words=="fucking"|words=="shit")) %>%
  # Add positive/negative status
  mutate(
    Sentiment = ifelse(Polarity > 0, "Positive", "Negative")
  )

# Plot polarity vs. (term reordered by polarity), filled by pos_or_neg
 polarity <- ggplot(data_tidy_pol, aes(reorder(words, Polarity), Polarity, fill = Sentiment)) +
  geom_col() + 
  ggtitle("Social Distancing Tweets: Sentiment Polarity by Word") + 
  # Rotate text and vertically justify
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, size = 10))+
  xlab("Word") 


save_plot("polarity_chart2.png", polarity)

```

```{r Sentiment Analysis - Chart 2: Most common pos and neg words}

word_counts <- virus_tibble_clean %>%
  # Remove swear words
  filter(!(words=="fuck"|words=="fucking"|words=="shit")) %>%
  # Implement sentiment analysis using the "bing" lexicon
  inner_join(get_sentiments("bing"), by = c("words" = "word")) %>%
  # Count by word and sentiment
  count(words, sentiment)

top_words <- word_counts %>%
  # Group by sentiment
  group_by(sentiment) %>%
  # Take the top 20 for each sentiment
  top_n(20) %>%
  ungroup() %>%
  
  # Make word a factor in order of n
  mutate(Words = reorder(words, n))

# Use aes() to put words on the x-axis and n on the y-axis
freq <- ggplot(top_words, aes(words, n, fill = sentiment)) +
 # Make a bar chart with geom_col()
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = n, hjust=1), size = 3.5, color = "black") +
  facet_wrap(~sentiment, scales = "free") +  
  coord_flip() +
  ggtitle("Most Common Positive and Negative words") + theme_classic()

save_plot("freq_chart.png", freq)
```

