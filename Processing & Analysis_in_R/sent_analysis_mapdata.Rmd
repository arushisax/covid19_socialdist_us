---
title: "sentiment_analysis_mapdata"
author: "Arushi Saxena"
date: "4/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(twitteR)
library(rtweet)
library(SnowballC)
library(tm)
library(syuzhet)
library(maps)
library(ggthemes)
library(revgeo)
library(mgsub)
library(lubridate)
```


```{r Tweet Sentiment Analysis}
# Text contains a lot of URLs, hashtags and other twitter handles. We will remove all these using the gsub function.

# Convert original CSV into data for sentiment analysis
senti <- read_csv("Shiny_app/tweets_0412_v2.csv")
head(senti)

head(senti$text)
senti <- gsub("http.*","",senti$text)
senti <- gsub("https.*","",senti)
senti <- gsub("#.*","",senti)
senti <- gsub("@.*","",senti)

# Getting sentiment defining words
word <- as.vector(senti)
emotion <- get_nrc_sentiment(word)
emotion2 <- cbind(senti, emotion) 

head(emotion2)

# Getting most positive sentiment
sent.value <- get_sentiment(word)
most.positive <- word[sent.value == max(sent.value)]
most.positive

# Getting most negative sentiment
most.negative <- word[sent.value <= min(sent.value)] 
most.negative 

# Make a tibble of scores by Twitter row ID
senti
sent.value

sentiment_scores <- tibble(manual_id = rep(1:18000),sent.value) 

sentiment_scores

# Add Twitter Row IDs to original df
twitter_sentiment_table <- read_csv("Shiny_app/tweets_0412_v2.csv") %>%
  mutate(manual_id = rep(1:18000)) %>%
  select(manual_id,text,country,geo_coords,coords_coords,bbox_coords,location)

twitter_sentiment_table

# Join sentiment score to the Twitter Sentiment Table on manual_id

twitter_sentiment_table2 <- left_join(twitter_sentiment_table, sentiment_scores, by = "manual_id")

head(twitter_sentiment_table2)

# Sub-select just the US data and US lats / longitudes. This assumes that people are tweeting from where they live, but might not always be the case
  
us_senti_locations <- senti_locations %>%
  filter(lat != "" & lng !="") %>%
  filter(country == "United States")

saveRDS(us_senti_locations, file = "us_senti_locations.rds")

# Only look at social distancing data after 3/11 (COVID-19 declared as pandemic) and average the values accross counties and categories to get an average state level view.

# Add Washington DC data manually from another source because it was missing in this web scraping

socialdistance_march <- us2 %>%
  mutate(date_formatted = ymd(date)) %>%
  filter(date_formatted >= "2020-03-11") %>%
  group_by(state) %>%
  summarize(average = round(mean(value),digits = 1)) %>%
  add_row(state = "District of Columbia", average = -17.3) %>%
  transform(average = as.numeric(average)) %>%
  arrange(average) 

# Convert Average column to a numeric column
socialdistance_march_F <- transform(socialdistance_march, average = as.numeric(average)) 
socialdistance_march_F

head(us2)
```

